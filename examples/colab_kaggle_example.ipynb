{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3-1B Reasoning Model - Colab/Kaggle Example\n",
    "\n",
    "This notebook demonstrates how to use the Gemma3-1B reasoning model in Google Colab or Kaggle environments.\n",
    "\n",
    "The project uses `etils[ecolab]` for automatic environment detection and configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if not already cloned)\n",
    "!git clone https://github.com/YOUR_USERNAME/ee596-fp.git\n",
    "%cd ee596-fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Notebook Environment with ecolab\n",
    "\n",
    "The `setup_notebook_env()` function automatically:\n",
    "- Detects if you're running in Colab or Kaggle\n",
    "- Configures plotting for better visualization\n",
    "- Sets up environment-specific optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import setup_notebook_env, get_device\n",
    "\n",
    "# Setup notebook environment\n",
    "setup_notebook_env()\n",
    "\n",
    "# Detect best device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Authenticate with HuggingFace\n",
    "\n",
    "Gemma is a gated model. You need to:\n",
    "1. Accept the license at [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)\n",
    "2. Get your token from [HuggingFace Settings](https://huggingface.co/settings/tokens)\n",
    "3. Login below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to HuggingFace\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Model\n",
    "\n",
    "The model will automatically use GPU if available in Colab/Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import load_model\n",
    "\n",
    "# Load model (base model without fine-tuning)\n",
    "# For GPU environments, you can use 4-bit quantization to save memory\n",
    "model = load_model(\n",
    "    checkpoint_path=None,  # Set to path of fine-tuned weights if you have them\n",
    "    device=\"auto\",\n",
    "    load_in_4bit=True if device == \"cuda\" else False  # Use quantization on GPU\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Model\n",
    "\n",
    "Let's test the model with a simple math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "question = \"A store sells apples for $2 each and oranges for $3 each. If Sarah buys 4 apples and 5 oranges, how much does she spend in total?\"\n",
    "\n",
    "# Generate answer\n",
    "result = model.solve(question, temperature=0.7)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"QUESTION:\")\n",
    "print(question)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REASONING:\")\n",
    "print(result[\"reasoning\"])\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Question Answering\n",
    "\n",
    "Try your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    \"\"\"Helper function to answer a question and display results.\"\"\"\n",
    "    result = model.solve(question, temperature=0.7)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“ REASONING:\")\n",
    "    print(result[\"reasoning\"] or \"(No reasoning found)\")\n",
    "    print(\"\\nâœ… ANSWER:\")\n",
    "    print(result[\"answer\"] or \"(No answer found)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try different questions\n",
    "questions = [\n",
    "    \"If a train travels at 60 mph for 2.5 hours, how far does it travel?\",\n",
    "    \"Why does ice float on water?\",\n",
    "    \"What is 15% of 200?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"\\n\\n### Question {i}: {q}\")\n",
    "    answer_question(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Information\n",
    "\n",
    "Check what environment we're running in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from etils import ecolab\n",
    "    \n",
    "    print(\"Environment Information:\")\n",
    "    print(f\"- Is Notebook: {ecolab.is_notebook()}\")\n",
    "    \n",
    "    if ecolab.is_notebook():\n",
    "        print(f\"- Notebook Name: {ecolab.get_notebook_name()}\")\n",
    "    \n",
    "    if hasattr(ecolab, 'is_colab') and ecolab.is_colab():\n",
    "        print(\"- Platform: Google Colab\")\n",
    "    elif hasattr(ecolab, 'is_kaggle') and ecolab.is_kaggle():\n",
    "        print(\"- Platform: Kaggle\")\n",
    "    else:\n",
    "        print(\"- Platform: Other notebook environment\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"ecolab not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing\n",
    "\n",
    "Process multiple questions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of questions\n",
    "test_questions = [\n",
    "    \"What is 25 * 4?\",\n",
    "    \"How many days are in a leap year?\",\n",
    "    \"If a rectangle has length 10 and width 5, what is its area?\",\n",
    "]\n",
    "\n",
    "# Process all questions\n",
    "results = []\n",
    "for q in test_questions:\n",
    "    result = model.solve(q, temperature=0.3)  # Lower temperature for more deterministic answers\n",
    "    results.append({\n",
    "        \"Question\": q,\n",
    "        \"Answer\": result[\"answer\"],\n",
    "        \"Reasoning\": result[\"reasoning\"][:100] + \"...\" if len(result[\"reasoning\"]) > 100 else result[\"reasoning\"]\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try more complex reasoning problems\n",
    "- Fine-tune the model on your own dataset\n",
    "- Evaluate on standard benchmarks like GSM8K\n",
    "- Experiment with different temperature and sampling settings\n",
    "\n",
    "For more information, see the [main README](../README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
