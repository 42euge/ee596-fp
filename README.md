# Gemma3-1B Reasoning Model with GRPO Fine-tuning

This project fine-tunes Google's Gemma3-1B model using Group Relative Policy Optimization (GRPO) to improve step-by-step reasoning capabilities. The model learns to produce structured reasoning traces with explicit `<reasoning>` and `<answer>` sections.

## Project Overview

**Goal**: Enhance Gemma3-1B's reasoning abilities through reinforcement learning, training it to:
- Think through problems step-by-step
- Show clear reasoning processes
- Provide structured answers

**Approach**:
- **Base Model**: Gemma3-1B-IT (instruction-tuned)
- **Fine-tuning Method**: GRPO with LoRA adapters
- **Training Data**: OpenRubrics dataset with rubric-based reward signals
- **Reward Functions**: Rubric-as-Reward (RaR) scoring + format compliance

## ğŸš€ Automated Development Pipelines

This project includes comprehensive automation for the entire reward model development lifecycle:

- âœ… **One-command setup** with `make quickstart`
- âœ… **Automated dataset preparation** with validation
- âœ… **Training orchestration** on local or TPU with W&B tracking
- âœ… **Automated evaluation** with comprehensive metrics
- âœ… **One-click deployment** to HuggingFace Hub
- âœ… **Real-time monitoring** with training dashboards
- âœ… **CI/CD integration** with GitHub Actions
- âœ… **Code quality checks** with pre-commit hooks

**Quick Start:**
```bash
# Full setup (install dependencies, prepare dataset)
make quickstart

# Start training
make train

# Evaluate model
make evaluate

# Monitor training
make monitor RUN=<run_name>

# Deploy checkpoint
make deploy CHECKPOINT=./checkpoints/step_1000 REPO_ID=username/model
```

**Documentation:**
- ğŸ“– [Complete Pipeline Guide](docs/PIPELINE_GUIDE.md) - Full documentation
- ğŸ“‹ [Quick Reference](docs/QUICK_REFERENCE.md) - Cheat sheet
- ğŸ”§ [CI/CD Setup](docs/CICD_SETUP.md) - GitHub Actions setup

## Repository Structure

```
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ Makefile               # Development automation (make quickstart, make train, etc.)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .pre-commit-config.yaml # Code quality hooks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py       # Package initialization
â”‚   â”œâ”€â”€ main.py           # Entry point for inference/evaluation
â”‚   â”œâ”€â”€ model.py          # Model loading and inference code
â”‚   â”œâ”€â”€ config.py         # Hyperparameters and configuration
â”‚   â””â”€â”€ utils.py          # Helper functions (data loading, rewards, etc.)
â”œâ”€â”€ scripts/               # Automation pipelines
â”‚   â”œâ”€â”€ reward_pipeline.py    # Main CLI for all pipelines
â”‚   â”œâ”€â”€ prepare_dataset.py    # Dataset preparation automation
â”‚   â”œâ”€â”€ train_grpo.py         # GRPO training script
â”‚   â”œâ”€â”€ evaluate_model.py     # Model evaluation automation
â”‚   â”œâ”€â”€ deploy_checkpoint.py  # HuggingFace deployment automation
â”‚   â”œâ”€â”€ monitor_training.py   # Training metrics dashboard
â”‚   â””â”€â”€ setup_tpu_vm.sh       # TPU environment setup
â”œâ”€â”€ TunRex/                # Dataset toolkit (git subtree)
â”‚   â””â”€â”€ src/tunrex/datasets/  # Dataset loading, rewards, evaluation
â”œâ”€â”€ .github/workflows/     # CI/CD automation
â”‚   â”œâ”€â”€ auto-evaluation.yml      # Automated evaluation on PRs
â”‚   â”œâ”€â”€ tpu-training.yml         # Quick TPU validation
â”‚   â””â”€â”€ tpu-training-full.yml    # Full TPU training
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PIPELINE_GUIDE.md  # Complete pipeline documentation
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md # Quick reference cheat sheet
â”‚   â””â”€â”€ CICD_SETUP.md      # GitHub Actions setup guide
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ demo.py           # Interactive demo script
â”œâ”€â”€ data/                  # Prepared datasets (generated)
â”œâ”€â”€ checkpoints/           # Saved model weights (generated)
â””â”€â”€ logs/                  # Training logs and evaluation results (generated)
```

## Setup Instructions

### 1. Clone the Repository
```bash
git clone <repository-url>
cd ee596-fp
```

### 2. Install Dependencies

**Using uv (recommended):**
```bash
uv sync
```

To activate the virtual environment:
```bash
source .venv/bin/activate
```

Or run commands directly without activating:
```bash
uv run python demo/demo.py
```

**Using pip:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Authenticate with HuggingFace (Required)

Gemma is a gated model. You must accept the license and authenticate:

1. Visit [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it) and accept the license
2. Create a HuggingFace token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
3. Login via CLI:
```bash
huggingface-cli login
```

### 4. Download Pre-trained Model (Optional)

If you have fine-tuned LoRA weights, place them in the `checkpoints/` directory:
```
checkpoints/
â””â”€â”€ lora/
    â”œâ”€â”€ adapter_config.json
    â””â”€â”€ adapter_model.safetensors
```

**Pre-trained Model Link**: [TODO: Add Google Drive/HuggingFace link]

## How to Run

### Interactive Demo

Run the demo script for an interactive reasoning session:

```bash
python demo/demo.py
```

With fine-tuned checkpoint:
```bash
python demo/demo.py --checkpoint ./checkpoints/lora
```

### Run Example Problems

See the model solve example problems across different categories:

```bash
python demo/demo.py --examples
```

### Device Selection

The demo automatically detects the best available device (CUDA > MPS > CPU). You can override this:

```bash
# Force CPU
python demo/demo.py --device cpu

# Force CUDA (if available)
python demo/demo.py --device cuda

# Force MPS (Apple Silicon)
python demo/demo.py --device mps
```

### Evaluation on GSM8K

Evaluate the model on the GSM8K math benchmark:

```bash
python -m src.main --mode evaluate --num-samples 100 --output results/eval.json
```

## Expected Output

When you run the demo, you should see output like:

```
======================================================================
  Gemma3-1B Reasoning Model - Demo
  Fine-tuned with GRPO for improved step-by-step reasoning
======================================================================

ğŸ–¥ï¸  Device: mps
ğŸ“ Using base model (no fine-tuned checkpoint)

â³ Loading model (this may take a minute)...
âœ… Model loaded successfully!

ğŸ’¬ Interactive Mode
   Enter your questions below. Type 'quit' or 'exit' to stop.

â“ Your question: How many apples does John have if he starts with 5 and buys 3 more?

â³ Thinking...

----------------------------------------------------------------------
Question: How many apples does John have if he starts with 5 and buys 3 more?
----------------------------------------------------------------------

ğŸ“ REASONING:
   Let's solve this step by step. John starts with 5 apples.
   He buys 3 more apples. To find the total number of apples,
   we add: 5 + 3 = 8 apples.

âœ… ANSWER:
   8

----------------------------------------------------------------------
```

## Configuration

Key hyperparameters in `src/config.py`:

| Parameter | Value | Description |
|-----------|-------|-------------|
| LoRA Rank | 64 | Low-rank adaptation dimension |
| LoRA Alpha | 64.0 | Scaling factor for LoRA |
| Learning Rate | 3e-6 | Training learning rate |
| Temperature | 0.9 | Generation temperature during training |
| Beta (KL) | 0.08 | KL divergence penalty coefficient |
| Max Generation | 512 | Maximum tokens to generate |

## Training (Advanced)

Training requires a TPU environment (Google Colab or Kaggle recommended). Use the training notebook at `demo/train_colab.ipynb` for the full training pipeline using Google's Tunix library.

Key training components:
- **GRPO**: Group Relative Policy Optimization for RL fine-tuning
- **Rubric-as-Reward**: Uses rubric overlap and reference similarity for reward signals
- **LoRA**: Parameter-efficient fine-tuning with low-rank adapters

### Creating and Using Checkpoints

The training notebook automatically saves checkpoints during training:

**Checkpoint Configuration:**
- Checkpoints are saved every 100 training steps via Orbax CheckpointManager
- The 3 most recent checkpoints are kept (`max_to_keep=3`)
- Set `SAVE_TO_DRIVE=True` in the notebook to persist checkpoints to Google Drive
- Checkpoints are saved to: `{CHECKPOINT_DIR}/actor/{step}/`

**To use checkpoints locally:**

1. After training completes, download `checkpoint_export.zip` from Google Drive (if using `SAVE_TO_DRIVE=True`)
2. Extract to your local `checkpoints/` directory:
   ```bash
   unzip checkpoint_export.zip -d checkpoints/
   ```
3. Run the demo with your checkpoint:
   ```bash
   python demo/demo.py --checkpoint ./checkpoints/actor/<step>/model_params
   ```

**Checkpoint Directory Structure:**
```
checkpoints/
â””â”€â”€ actor/
    â”œâ”€â”€ 100/
    â”‚   â””â”€â”€ model_params/
    â”œâ”€â”€ 200/
    â”‚   â””â”€â”€ model_params/
    â””â”€â”€ 300/
        â””â”€â”€ model_params/
```

## Model Architecture

- **Base Model**: Gemma3-1B-IT (1 billion parameters)
- **Architecture**: Decoder-only transformer
- **Fine-tuning**: LoRA adapters on attention layers
- **Prompt Format**: Gemma chat template with `<start_of_turn>` tokens

## Acknowledgments

- **Google DeepMind**: Gemma3 model and Tunix training library
- **OpenRubrics Dataset**: Training data with rubric-based evaluations
- **GSM8K Dataset**: Math reasoning evaluation benchmark
- **GRPO Paper**: Group Relative Policy Optimization methodology

## References

- [Gemma Model Card](https://ai.google.dev/gemma)
- [GRPO Paper](https://arxiv.org/abs/2402.03300)
- [Rubric-as-Reward Paper](https://arxiv.org/pdf/2507.17746)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
