name: GRPO Stability Issue Demo

# Demonstrates the problems with GRPO when using continuous/subjective rewards
# This workflow validates the RLOO feature request by showing:
# 1. Std normalization amplifies noise with continuous rewards
# 2. Lack of KL mechanism leads to policy drift
# 3. Training instability with rubric-based scoring

on:
  workflow_dispatch:
    inputs:
      num_steps:
        description: 'Number of training steps for demo'
        default: '20'
        type: string
      tpu_type:
        description: 'TPU accelerator type'
        default: 'v5litepod-4'
        type: choice
        options:
          - v5litepod-1
          - v5litepod-4
          - v5litepod-8
  # Also run on PRs that modify reward or GRPO code
  pull_request:
    paths:
      - 'src/llm_judge/rewards.py'
      - 'src/rubrics/**'
      - 'scripts/train_grpo.py'
      - 'scripts/demonstrate_grpo_instability.py'
      - 'scripts/reward_utils.py'

concurrency:
  group: grpo-stability-demo-${{ github.ref }}
  cancel-in-progress: true

env:
  TPU_NAME: demo-stability-${{ github.run_id }}
  GCP_PROJECT: kaggle-euge
  TPU_TYPE: ${{ github.event.inputs.tpu_type || 'v5litepod-4' }}
  NUM_STEPS: ${{ github.event.inputs.num_steps || '20' }}
  TPU_ZONES: "us-central1-a us-east1-d us-east5-b us-south1-a europe-west4-b"

jobs:
  demonstrate-instability:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT }}

      - name: Create TPU VM
        id: create-tpu
        run: |
          echo "ðŸš€ Creating TPU VM for GRPO stability demonstration"
          echo "TPU: $TPU_NAME (type: $TPU_TYPE)"
          echo "Will try zones: $TPU_ZONES"

          for zone in $TPU_ZONES; do
            echo ""
            echo "Attempting zone: $zone"
            if gcloud compute tpus tpu-vm create $TPU_NAME \
              --project=$GCP_PROJECT \
              --zone=$zone \
              --accelerator-type=$TPU_TYPE \
              --version=v2-alpha-tpuv5-lite \
              --preemptible 2>&1; then
              echo "âœ… TPU VM created successfully in $zone"
              echo "tpu_zone=$zone" >> $GITHUB_OUTPUT
              echo "tpu_created=true" >> $GITHUB_OUTPUT
              exit 0
            else
              echo "âŒ Failed to create TPU in $zone, trying next zone..."
            fi
          done

          echo "ERROR: Failed to create TPU VM in any zone"
          exit 1

      - name: Wait for TPU VM to be ready
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          echo "â³ Waiting for TPU VM to be ready in $TPU_ZONE..."
          sleep 30

          for i in {1..10}; do
            if gcloud compute tpus tpu-vm ssh $TPU_NAME \
              --project=$GCP_PROJECT \
              --zone=$TPU_ZONE \
              --command="echo 'TPU VM is ready'" 2>/dev/null; then
              echo "âœ… TPU VM is ready!"
              break
            fi
            echo "Attempt $i: TPU VM not ready yet, waiting..."
            sleep 15
          done

      - name: Copy code to TPU VM
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          tar -czf /tmp/repo.tar.gz -C $GITHUB_WORKSPACE .

          gcloud compute tpus tpu-vm scp /tmp/repo.tar.gz $TPU_NAME:~/repo.tar.gz \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE

          gcloud compute tpus tpu-vm ssh $TPU_NAME \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE \
            --command="mkdir -p ~/training && tar -xzf ~/repo.tar.gz -C ~/training"

      - name: Setup TPU VM environment
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          gcloud compute tpus tpu-vm ssh $TPU_NAME \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE \
            --command="cd ~/training && bash scripts/setup_tpu_vm.sh"

      - name: Configure HuggingFace auth
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          echo "${HF_TOKEN}" > /tmp/hf_token.txt
          gcloud compute tpus tpu-vm scp /tmp/hf_token.txt $TPU_NAME:~/hf_token.txt \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE
          rm /tmp/hf_token.txt

          gcloud compute tpus tpu-vm ssh $TPU_NAME \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE \
            --command="mkdir -p ~/.cache/huggingface && mv ~/hf_token.txt ~/.cache/huggingface/token"

      - name: Run GRPO stability demonstration
        id: run-demo
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          echo "ðŸ§ª Running GRPO stability demonstration with continuous rewards"
          echo "This will show:"
          echo "  1. Std normalization amplifying noise"
          echo "  2. Policy drift without KL constraint"
          echo "  3. Training instability with subjective rewards"
          echo ""

          # Run the demonstration script
          gcloud compute tpus tpu-vm ssh $TPU_NAME \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE \
            --command="cd ~/training && uv run python scripts/demonstrate_grpo_instability.py --num-steps $NUM_STEPS --no-wandb" \
            | tee /tmp/demo_output.txt || true

          # Check if warnings were detected
          if grep -q "WARNING: High reward std" /tmp/demo_output.txt; then
            echo "detected_high_std=true" >> $GITHUB_OUTPUT
          fi
          if grep -q "WARNING: High KL divergence" /tmp/demo_output.txt; then
            echo "detected_high_kl=true" >> $GITHUB_OUTPUT
          fi

      - name: Retrieve metrics from TPU
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          echo "ðŸ“Š Retrieving stability metrics..."

          gcloud compute tpus tpu-vm scp $TPU_NAME:/tmp/grpo_stability_metrics.json /tmp/grpo_stability_metrics.json \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE || echo "âš ï¸  Metrics file not found (training may have failed)"

      - name: Upload stability metrics as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: grpo-stability-metrics
          path: /tmp/grpo_stability_metrics.json
          retention-days: 30

      - name: Analyze and report findings
        if: always()
        run: |
          echo "## ðŸ” GRPO Stability Analysis Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "This demonstration validates the RLOO feature request by showing issues with GRPO on continuous rewards." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f /tmp/grpo_stability_metrics.json ]; then
            echo "### Metrics Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract key metrics using jq
            if command -v jq &> /dev/null; then
              mean_std=$(jq '[.reward_std[]] | add / length' /tmp/grpo_stability_metrics.json)
              max_std=$(jq '[.reward_std[]] | max' /tmp/grpo_stability_metrics.json)
              max_kl=$(jq '[.kl_divergence[] | select(. != null)] | max // 0' /tmp/grpo_stability_metrics.json)

              echo "- **Mean Reward Std**: $mean_std" >> $GITHUB_STEP_SUMMARY
              echo "- **Max Reward Std**: $max_std" >> $GITHUB_STEP_SUMMARY
              echo "- **Max KL Divergence**: $max_kl" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY

              # Check if issues were detected
              high_std=$(echo "$max_std > 2.0" | bc -l)
              high_kl=$(echo "$max_kl > 0.1" | bc -l)

              if [ "$high_std" -eq 1 ] || [ "$high_kl" -eq 1 ]; then
                echo "### âŒ Issues Detected" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY

                if [ "$high_std" -eq 1 ]; then
                  echo "- **High Reward Variance**: Std normalization amplifies noise in continuous rewards" >> $GITHUB_STEP_SUMMARY
                fi

                if [ "$high_kl" -eq 1 ]; then
                  echo "- **Policy Drift**: KL divergence indicates policy is drifting from reference" >> $GITHUB_STEP_SUMMARY
                fi

                echo "" >> $GITHUB_STEP_SUMMARY
                echo "### ðŸ’¡ Recommendation" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "These issues support the need for **RLOO (REINFORCE Leave-One-Out)** as proposed in the feature request:" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "- RLOO uses \`A_i = R_i - mean(R_j where j != i)\` instead of std normalization" >> $GITHUB_STEP_SUMMARY
                echo "- RLOO integrates KL directly into reward: \`R'_i = R_i - Î² * KL\`" >> $GITHUB_STEP_SUMMARY
                echo "- More robust to noisy/subjective rewards (Ahmadian et al. 2024)" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "**References**:" >> $GITHUB_STEP_SUMMARY
                echo "- [verl RLOO implementation](https://github.com/volcengine/verl)" >> $GITHUB_STEP_SUMMARY
                echo "- [swift RLOO docs](https://github.com/modelscope/swift)" >> $GITHUB_STEP_SUMMARY
              else
                echo "### âœ… No significant issues detected" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "Try increasing steps or noise level to reproduce the issue." >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "âš ï¸  jq not available for metrics analysis" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### âš ï¸  No metrics available" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The demonstration script may have failed. Check the logs for details." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "Download the full metrics JSON from the workflow artifacts for detailed analysis." >> $GITHUB_STEP_SUMMARY

      - name: Cleanup TPU VM
        if: always()
        env:
          TPU_ZONE: ${{ steps.create-tpu.outputs.tpu_zone }}
        run: |
          echo "ðŸ§¹ Cleaning up TPU VM: $TPU_NAME in $TPU_ZONE"
          gcloud compute tpus tpu-vm delete $TPU_NAME \
            --project=$GCP_PROJECT \
            --zone=$TPU_ZONE \
            --quiet || echo "TPU VM cleanup failed or VM doesn't exist"

  # Alternative: Local simulation (no TPU required)
  simulate-instability:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: |
          uv sync --extra test

      - name: Run simplified stability simulation
        run: |
          # Create a simple simulation script that doesn't need TPU
          cat > /tmp/simulate_grpo_issue.py << 'EOF'
          """Simplified simulation of GRPO stability issues without TPU."""
          import numpy as np
          import json

          np.random.seed(42)

          # Simulate training steps
          num_steps = 50
          num_generations_per_step = 8

          metrics = {
              "step": [],
              "reward_mean": [],
              "reward_std": [],
              "advantage_std_amplification": [],
              "issues_detected": []
          }

          print("=" * 80)
          print("Simulating GRPO with Continuous/Subjective Rewards")
          print("=" * 80)

          for step in range(1, num_steps + 1):
              # Simulate continuous rubric scores (high variance)
              # Real scenario: LLM judge gives subjective scores 0-10
              base_score = 5.0
              noise_std = 2.5  # High variance in subjective evaluation
              rewards = np.random.normal(base_score, noise_std, num_generations_per_step)
              rewards = np.clip(rewards, 0, 10)

              # GRPO advantage computation
              reward_mean = np.mean(rewards)
              reward_std = np.std(rewards)

              # This is the problem: std normalization
              advantages = (rewards - reward_mean) / (reward_std + 1e-8)

              # Measure amplification
              advantage_std = np.std(advantages)
              amplification = advantage_std / (reward_std + 1e-8)

              metrics["step"].append(step)
              metrics["reward_mean"].append(float(reward_mean))
              metrics["reward_std"].append(float(reward_std))
              metrics["advantage_std_amplification"].append(float(amplification))

              # Detect issues
              issues = []
              if reward_std > 2.0:
                  issues.append("high_reward_variance")
                  print(f"âš ï¸  Step {step}: High reward std = {reward_std:.3f}")

              if amplification > 1.5:
                  issues.append("std_amplification")

              metrics["issues_detected"].append(issues)

          # Save results
          with open('/tmp/simulation_results.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          # Summary
          print("\n" + "=" * 80)
          print("SIMULATION SUMMARY")
          print("=" * 80)
          print(f"Mean reward std: {np.mean(metrics['reward_std']):.3f}")
          print(f"Max reward std: {np.max(metrics['reward_std']):.3f}")

          high_var_steps = sum(1 for std in metrics['reward_std'] if std > 2.0)
          print(f"\nSteps with high variance: {high_var_steps}/{num_steps}")

          if high_var_steps > 0:
              print("\nâŒ ISSUE CONFIRMED: GRPO's std normalization amplifies noise")
              print("   in continuous/subjective rewards")
              print("\nðŸ’¡ RLOO would address this by using:")
              print("   A_i = R_i - mean(R_j where j != i)  # No std division")

          print("=" * 80)
          EOF

          uv run python /tmp/simulate_grpo_issue.py

      - name: Upload simulation results
        uses: actions/upload-artifact@v4
        with:
          name: grpo-simulation-results
          path: /tmp/simulation_results.json
          retention-days: 30

      - name: Create issue summary
        run: |
          echo "## ðŸ“Š GRPO Stability Simulation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "This simulation demonstrates why RLOO is needed for continuous/subjective rewards." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f /tmp/simulation_results.json ]; then
            echo "### Key Findings" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âœ… **Simulation completed successfully**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The simulation shows how GRPO's advantage computation:" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`python" >> $GITHUB_STEP_SUMMARY
            echo "A_i = (R_i - mean(R)) / std(R)  # GRPO" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "amplifies noise when rewards have high variance (subjective evaluation)." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**RLOO alternative:**" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`python" >> $GITHUB_STEP_SUMMARY
            echo "A_i = R_i - mean(R_j where j != i)  # RLOO (no std normalization)" >> $GITHUB_STEP_SUMMARY
            echo "R'_i = R_i - Î² * KL                 # KL integrated into reward" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### References" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Ahmadian et al. 2024: [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740)" >> $GITHUB_STEP_SUMMARY
            echo "- [verl RLOO implementation](https://github.com/volcengine/verl)" >> $GITHUB_STEP_SUMMARY
            echo "- [swift RLOO docs](https://github.com/modelscope/swift)" >> $GITHUB_STEP_SUMMARY
          fi
