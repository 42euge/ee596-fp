{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Methodology Comparison Analysis\n",
    "\n",
    "This notebook demonstrates how to compare different grading methodologies and analyze their effects on model behavior during GRPO training.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll explore:\n",
    "1. **Different grading methods** - Format, accuracy, and rubric-based rewards\n",
    "2. **Statistical comparison** - How methods differ in score distributions\n",
    "3. **Correlation analysis** - Which methods agree/disagree\n",
    "4. **Behavior effects** - How grading influences model outputs\n",
    "5. **Visualization** - Plots to understand the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from grading_registry import create_standard_comparator, get_all_grading_methods\n",
    "from utils import load_gsm8k_dataset, load_openrubrics_dataset\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Available Grading Methods\n",
    "\n",
    "Let's see what grading methods are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = get_all_grading_methods()\n",
    "\n",
    "print(\"Available Grading Methods:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, metadata in methods.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Description: {metadata['description']}\")\n",
    "    print(f\"  Score range: {metadata['score_range']}\")\n",
    "    print(f\"  Requires ground truth: {metadata['requires_ground_truth']}\")\n",
    "    print(f\"  Requires rubric: {metadata['requires_rubric']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "We'll use the GSM8K math dataset for this analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    dataset = load_gsm8k_dataset(split=\"train\", max_examples=200, seed=42)\n",
    "    print(f\"Loaded {len(dataset)} examples from GSM8K\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample question:\")\n",
    "    print(dataset[0]['question'])\n",
    "    print(f\"\\nAnswer: {dataset[0]['answer']}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease download the GSM8K dataset from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/thedevastator/grade-school-math-8k-q-a\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic model completions with varying quality\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "if dataset:\n",
    "    for item in dataset:\n",
    "        # Simulate different quality levels\n",
    "        quality = random.choice(['good', 'good', 'partial', 'poor'])  # Bias toward good\n",
    "        \n",
    "        if quality == 'good':\n",
    "            # Proper format + correct answer\n",
    "            item['completion'] = (\n",
    "                f\"<reasoning>Let me solve this step by step. \"\n",
    "                f\"After careful calculation, the answer is {item['answer']}.</reasoning>\"\n",
    "                f\"<answer>{item['answer']}</answer>\"\n",
    "            )\n",
    "        elif quality == 'partial':\n",
    "            # Proper format + wrong answer\n",
    "            wrong_answer = str(int(item['answer']) + random.randint(-10, 10))\n",
    "            item['completion'] = (\n",
    "                f\"<reasoning>Working through this problem...</reasoning>\"\n",
    "                f\"<answer>{wrong_answer}</answer>\"\n",
    "            )\n",
    "        else:\n",
    "            # Poor format + wrong answer\n",
    "            item['completion'] = f\"I think the answer is {random.randint(0, 100)}\"\n",
    "    \n",
    "    print(\"Generated synthetic completions\")\n",
    "    print(\"\\nExample completion:\")\n",
    "    print(dataset[0]['completion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Comparison\n",
    "\n",
    "Now let's compare different grading methods on this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset:\n",
    "    # Prepare data\n",
    "    prompts = [item['question'] for item in dataset]\n",
    "    completions = [item['completion'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "    \n",
    "    # Create comparator\n",
    "    comparator = create_standard_comparator()\n",
    "    \n",
    "    # Run comparison\n",
    "    results = comparator.compare(\n",
    "        prompts=prompts,\n",
    "        completions=completions,\n",
    "        method_names=['format_reward', 'accuracy_reward'],\n",
    "        answers=answers\n",
    "    )\n",
    "    \n",
    "    print(\"Comparison complete!\")\n",
    "    print(f\"Compared {len(results.methods)} methods on {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis\n",
    "\n",
    "Let's examine the statistical properties of each grading method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results:\n",
    "    print(\"Statistical Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for method in results.methods:\n",
    "        stats = results.statistics[method]\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        print(f\"  Mean:     {stats['mean']:.3f}\")\n",
    "        print(f\"  Median:   {stats['median']:.3f}\")\n",
    "        print(f\"  Std Dev:  {stats['std']:.3f}\")\n",
    "        print(f\"  Min:      {stats['min']:.3f}\")\n",
    "        print(f\"  Max:      {stats['max']:.3f}\")\n",
    "        print(f\"  Q1 (25%): {stats['q25']:.3f}\")\n",
    "        print(f\"  Q3 (75%): {stats['q75']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "How do the methods correlate with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results:\n",
    "    print(\"Correlations Between Methods:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for pair, corr in results.correlations.items():\n",
    "        if 'pearson' in pair:\n",
    "            methods = pair.replace('_pearson', '').replace('_vs_', ' vs ')\n",
    "            print(f\"\\n{methods}:\")\n",
    "            print(f\"  Pearson correlation: {corr:.3f}\")\n",
    "        elif 'spearman' in pair:\n",
    "            spearman_corr = corr\n",
    "            print(f\"  Spearman correlation: {spearman_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### 6.1 Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results:\n",
    "    comparator.plot_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results and len(results.methods) > 1:\n",
    "    comparator.plot_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Pairwise Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results and len(results.methods) > 1:\n",
    "    comparator.plot_pairwise_scatter('format_reward', 'accuracy_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find Disagreements\n",
    "\n",
    "Let's find examples where the two methods disagree most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results and len(results.methods) > 1:\n",
    "    disagreements = comparator.find_disagreements(\n",
    "        'format_reward', 'accuracy_reward', top_k=5, normalize=True\n",
    "    )\n",
    "    \n",
    "    print(\"Top 5 Disagreements:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, ex in enumerate(disagreements, 1):\n",
    "        print(f\"\\n{i}. Difference: {ex['difference']:.3f}\")\n",
    "        print(f\"   Format reward score: {ex['format_reward_score']:.3f}\")\n",
    "        print(f\"   Accuracy reward score: {ex['accuracy_reward_score']:.3f}\")\n",
    "        print(f\"\\n   Question: {ex['prompt'][:150]}...\")\n",
    "        print(f\"\\n   Completion: {ex['completion'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Behavior Analysis\n",
    "\n",
    "How do different score ranges correlate with model behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results:\n",
    "    print(\"Behavior Analysis by Score Range:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for method in results.methods:\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        behavior = comparator.analyze_behavior_effects(method)\n",
    "        \n",
    "        for group_name, analysis in behavior.items():\n",
    "            print(f\"\\n  {group_name.upper()}:\")\n",
    "            print(f\"    Count: {analysis['count']}\")\n",
    "            print(f\"    Avg completion length: {analysis['avg_length']:.1f} chars\")\n",
    "            print(f\"    Avg word count: {analysis['avg_word_count']:.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n",
    "\n",
    "Save the analysis results for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset and results:\n",
    "    output_dir = Path.cwd().parent / \"results\" / \"notebook_analysis\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save results\n",
    "    results.save(str(output_dir / \"comparison_results.json\"))\n",
    "    \n",
    "    # Generate report\n",
    "    comparator.generate_report(str(output_dir / \"comparison_report.md\"))\n",
    "    \n",
    "    # Save plots\n",
    "    comparator.plot_distributions(str(output_dir / \"distributions.png\"))\n",
    "    if len(results.methods) > 1:\n",
    "        comparator.plot_correlation_heatmap(str(output_dir / \"correlations.png\"))\n",
    "        comparator.plot_pairwise_scatter(\n",
    "            'format_reward', 'accuracy_reward',\n",
    "            str(output_dir / \"scatter.png\")\n",
    "        )\n",
    "    \n",
    "    print(f\"Results saved to {output_dir}/\")\n",
    "    print(\"\\nSaved files:\")\n",
    "    for f in output_dir.iterdir():\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Analysis\n",
    "\n",
    "Use this cell to run your own custom analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n",
    "# For example, you could:\n",
    "# - Compare additional methods\n",
    "# - Analyze different datasets\n",
    "# - Create custom visualizations\n",
    "# - Test your own grading functions\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
