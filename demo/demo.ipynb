{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Gemma3-1B Reasoning Model Demo\n",
    "\n",
    "## Fine-tuned with GRPO using Google's Tunix Library\n",
    "\n",
    "This notebook demonstrates the capabilities of a Gemma3-1B model fine-tuned using **Group Relative Policy Optimization (GRPO)** for improved step-by-step reasoning.\n",
    "\n",
    "### What is GRPO?\n",
    "GRPO (Group Relative Policy Optimization) is a critic-free reinforcement learning algorithm that:\n",
    "- Compares multiple model generations to find the best response\n",
    "- Uses relative rankings instead of absolute reward values\n",
    "- Achieves ~12% improvement on math reasoning benchmarks (GSM8K)\n",
    "\n",
    "### Training Approach\n",
    "- **Base Model**: Gemma3-1B-IT (instruction-tuned)\n",
    "- **Training Library**: [Google Tunix](https://github.com/google/tunix)\n",
    "- **Reward Function**: Rubric-as-Reward (RaR) + Format Compliance\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install torch transformers peft accelerate safetensors bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.model import GemmaModel, get_device\n",
    "from src.config import format_prompt, get_system_prompt, SYSTEM_PROMPTS\n",
    "from src.utils import extract_reasoning_and_answer, detect_question_type\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the best available device\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check VRAM/RAM availability\n",
    "import torch\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif device == \"mps\":\n",
    "    print(\"Using Apple Silicon MPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-loading-header",
   "metadata": {},
   "source": [
    "## 2. Load the Model\n",
    "\n",
    "We'll load the Gemma3-1B model. You can optionally load fine-tuned LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHECKPOINT_PATH = None  # Set to \"../checkpoints/lora\" if you have fine-tuned weights\n",
    "USE_4BIT = False  # Set True for 4-bit quantization (CUDA only, reduces memory)\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading Gemma3-1B model...\")\n",
    "model = GemmaModel(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    device=device,\n",
    "    load_in_4bit=USE_4BIT and device == \"cuda\",\n",
    ")\n",
    "model.load()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-functions-header",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def display_result(question: str, result: dict, category: str = None):\n",
    "    \"\"\"Display a result with nice formatting.\"\"\"\n",
    "    md = f\"### Question\\n{question}\\n\\n\"\n",
    "    \n",
    "    if category:\n",
    "        md += f\"**Category**: {category}\\n\\n\"\n",
    "    \n",
    "    md += \"### Reasoning\\n\"\n",
    "    reasoning = result.get('reasoning', '')\n",
    "    if reasoning:\n",
    "        md += f\"{reasoning}\\n\\n\"\n",
    "    else:\n",
    "        md += \"*No reasoning section found*\\n\\n\"\n",
    "    \n",
    "    md += \"### Answer\\n\"\n",
    "    answer = result.get('answer', '')\n",
    "    if answer:\n",
    "        md += f\"**{answer}**\\n\"\n",
    "    else:\n",
    "        md += \"*No answer section found*\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "def solve_problem(question: str, temperature: float = 0.7, category: str = None):\n",
    "    \"\"\"Solve a problem and display the result.\"\"\"\n",
    "    result = model.solve(\n",
    "        question,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    display_result(question, result, category)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-header",
   "metadata": {},
   "source": [
    "## 4. Math Reasoning Examples\n",
    "\n",
    "These examples demonstrate the model's ability to solve math word problems step-by-step, similar to GSM8K benchmark problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"A store sells apples for $2 each and oranges for $3 each. \"\n",
    "    \"If Sarah buys 4 apples and 5 oranges, how much does she spend in total?\",\n",
    "    category=\"Math - Basic Arithmetic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"A train travels at 60 miles per hour. How far will it travel in 2.5 hours?\",\n",
    "    category=\"Math - Distance/Speed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-example-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning \"\n",
    "    \"and bakes muffins for her friends every day with four. She sells the remainder \"\n",
    "    \"at the farmers' market daily for $2 per fresh duck egg. How much in dollars \"\n",
    "    \"does she make every day at the farmers' market?\",\n",
    "    category=\"Math - Multi-step Word Problem\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-example-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"A bookstore has 120 books. They sell 35% of them in the first week and \"\n",
    "    \"25% of the remaining books in the second week. How many books are left?\",\n",
    "    category=\"Math - Percentages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logic-header",
   "metadata": {},
   "source": [
    "## 5. Logic and Deductive Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"If all cats have tails, and Whiskers is a cat, what can we conclude about Whiskers?\",\n",
    "    category=\"Logic - Syllogism\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"There are three boxes: one contains only apples, one contains only oranges, \"\n",
    "    \"and one contains both apples and oranges. The boxes are labeled, but all labels \"\n",
    "    \"are wrong. If you can only pick one fruit from one box to determine the contents \"\n",
    "    \"of all boxes, which box should you pick from and why?\",\n",
    "    category=\"Logic - Puzzle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic-example-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"In a room, there are 5 people. Each person shakes hands with every other person \"\n",
    "    \"exactly once. How many handshakes occur in total?\",\n",
    "    category=\"Logic - Combinatorics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "science-header",
   "metadata": {},
   "source": [
    "## 6. Science Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "science-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"Why does ice float on water instead of sinking?\",\n",
    "    category=\"Science - Physics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "science-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"What causes the sky to appear blue during the day?\",\n",
    "    category=\"Science - Optics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "science-example-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"Explain how photosynthesis works in simple terms.\",\n",
    "    category=\"Science - Biology\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domain-header",
   "metadata": {},
   "source": [
    "## 7. Domain-Specific Applications\n",
    "\n",
    "These examples showcase how the model can be used as a domain expert assistant, similar to Tunix's mobile deployment demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domain-medical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical Assistant Example\n",
    "solve_problem(\n",
    "    \"A patient presents with a fever of 101Â°F, sore throat, and swollen lymph nodes. \"\n",
    "    \"What are the possible conditions to consider and what initial tests might be helpful?\",\n",
    "    category=\"Medical - Differential Diagnosis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domain-legal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal Assistant Example\n",
    "solve_problem(\n",
    "    \"A tenant has not paid rent for 3 months. What are the general steps a landlord \"\n",
    "    \"should follow before proceeding with an eviction?\",\n",
    "    category=\"Legal - Landlord-Tenant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domain-coding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Assistant Example\n",
    "solve_problem(\n",
    "    \"I have a Python list of numbers and I want to find all pairs that sum to a target value. \"\n",
    "    \"What's an efficient approach to solve this problem?\",\n",
    "    category=\"Coding - Algorithm Design\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domain-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Analysis Example\n",
    "solve_problem(\n",
    "    \"A company has revenue of $1M, COGS of $400K, operating expenses of $300K, \"\n",
    "    \"and pays 25% in taxes. Calculate the net profit margin.\",\n",
    "    category=\"Finance - Profitability Analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-header",
   "metadata": {},
   "source": [
    "## 8. Creative Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"Imagine a world where plants could communicate with humans. \"\n",
    "    \"How might this change agriculture?\",\n",
    "    category=\"Creative - Hypothetical Scenario\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_problem(\n",
    "    \"If you could redesign the education system from scratch, \"\n",
    "    \"what would be the key principles you would incorporate?\",\n",
    "    category=\"Creative - System Design\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 9. Generation Strategy Comparison\n",
    "\n",
    "Compare different generation strategies to see how they affect output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"What is 15% of 80?\"\n",
    "\n",
    "strategies = {\n",
    "    \"Greedy (temp=0.01)\": {\"temperature\": 0.01, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    \"Standard (temp=0.7)\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    \"Creative (temp=0.9)\": {\"temperature\": 0.9, \"top_k\": 100, \"top_p\": 0.95},\n",
    "}\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, params in strategies.items():\n",
    "    print(f\"\\n### {name}\")\n",
    "    result = model.solve(test_question, **params)\n",
    "    print(f\"Reasoning: {result.get('reasoning', 'N/A')[:200]}...\")\n",
    "    print(f\"Answer: {result.get('answer', 'N/A')}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-header",
   "metadata": {},
   "source": [
    "## 10. Batch Processing\n",
    "\n",
    "Process multiple questions efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_questions = [\n",
    "    \"What is 7 + 8?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How many legs does a spider have?\",\n",
    "    \"What is 25% of 200?\",\n",
    "]\n",
    "\n",
    "print(\"Processing batch of questions...\\n\")\n",
    "\n",
    "for i, q in enumerate(batch_questions, 1):\n",
    "    print(f\"[{i}/{len(batch_questions)}] {q}\")\n",
    "    result = model.solve(q, temperature=0.7)\n",
    "    print(f\"   Answer: {result.get('answer', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-header",
   "metadata": {},
   "source": [
    "## 11. Interactive Mode\n",
    "\n",
    "Enter your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your question here\n",
    "YOUR_QUESTION = \"How many prime numbers are there between 1 and 20?\"\n",
    "\n",
    "if YOUR_QUESTION:\n",
    "    solve_problem(YOUR_QUESTION, category=\"Custom Question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grpo-header",
   "metadata": {},
   "source": [
    "## 12. Understanding GRPO Training\n",
    "\n",
    "Here's a conceptual overview of how GRPO (Group Relative Policy Optimization) works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "grpo_explanation = \"\"\"\n",
    "### GRPO Algorithm Overview\n",
    "\n",
    "**Group Relative Policy Optimization** is a critic-free RL algorithm:\n",
    "\n",
    "1. **Generate Groups**: For each prompt, generate N different responses\n",
    "   ```\n",
    "   responses = [model.generate(prompt) for _ in range(N)]\n",
    "   ```\n",
    "\n",
    "2. **Score Responses**: Use reward functions to score each response\n",
    "   ```\n",
    "   rewards = [\n",
    "       format_reward(r) +      # Check <reasoning> and <answer> tags\n",
    "       rubric_reward(r) +      # Rubric overlap scoring  \n",
    "       accuracy_reward(r)      # Correctness (if verifiable)\n",
    "   for r in responses]\n",
    "   ```\n",
    "\n",
    "3. **Compute Advantages**: Use relative ranking within the group\n",
    "   ```\n",
    "   advantages = (rewards - mean(rewards)) / std(rewards)\n",
    "   ```\n",
    "\n",
    "4. **Policy Update**: Update model to increase probability of better responses\n",
    "   ```\n",
    "   loss = -log_prob(best_responses) * advantages\n",
    "   ```\n",
    "\n",
    "### Key Benefits:\n",
    "- **No Critic Network**: Simpler than PPO, uses group comparisons\n",
    "- **Sample Efficient**: Learns from relative rankings\n",
    "- **Stable Training**: Avoids reward hacking through normalization\n",
    "\n",
    "### Training Configuration (from config.py):\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| LoRA Rank | 64 | Low-rank adaptation dimension |\n",
    "| Temperature | 0.9 | Sampling temperature during training |\n",
    "| Beta (KL) | 0.08 | KL divergence penalty |\n",
    "| Learning Rate | 3e-6 | Optimizer learning rate |\n",
    "| Num Generations | 2 | Responses per prompt |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(grpo_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## 13. Model Evaluation (GSM8K Sample)\n",
    "\n",
    "Evaluate the model on a sample of GSM8K-style problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation problems with ground truth\n",
    "eval_problems = [\n",
    "    {\"question\": \"If you have 3 apples and buy 5 more, how many apples do you have?\", \"answer\": \"8\"},\n",
    "    {\"question\": \"A book costs $15. How much do 4 books cost?\", \"answer\": \"60\"},\n",
    "    {\"question\": \"If a train travels 120 miles in 2 hours, what is its speed in miles per hour?\", \"answer\": \"60\"},\n",
    "    {\"question\": \"A pizza is cut into 8 slices. If you eat 3 slices, what fraction is left?\", \"answer\": \"5/8\"},\n",
    "    {\"question\": \"What is 20% of 50?\", \"answer\": \"10\"},\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "print(\"Evaluating on sample problems...\\n\")\n",
    "\n",
    "for i, prob in enumerate(eval_problems, 1):\n",
    "    result = model.solve(prob[\"question\"], temperature=0.3)\n",
    "    model_answer = result.get(\"answer\", \"\").strip()\n",
    "    expected = prob[\"answer\"]\n",
    "    \n",
    "    # Simple check (could be more sophisticated)\n",
    "    is_correct = expected in model_answer or model_answer in expected\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    status = \"\" if is_correct else \"\"\n",
    "    print(f\"[{i}] {prob['question']}\")\n",
    "    print(f\"    Expected: {expected} | Got: {model_answer} {status}\\n\")\n",
    "\n",
    "accuracy = correct / len(eval_problems) * 100\n",
    "print(f\"\\nAccuracy: {correct}/{len(eval_problems)} ({accuracy:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system-prompts-header",
   "metadata": {},
   "source": [
    "## 14. System Prompt Variations\n",
    "\n",
    "The model supports different system prompts that affect reasoning style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-prompts-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available System Prompts:\\n\")\n",
    "\n",
    "for version, prompt in SYSTEM_PROMPTS.items():\n",
    "    print(f\"Version {version}:\")\n",
    "    print(f\"  {prompt[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-prompt-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different system prompts\n",
    "test_q = \"What is 7 times 8?\"\n",
    "\n",
    "for version in [0, 2, 6]:\n",
    "    print(f\"\\n=== System Prompt Version {version} ===\")\n",
    "    result = model.solve(test_q, system_prompt_version=version, temperature=0.5)\n",
    "    print(f\"Answer: {result.get('answer', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Model Loading**: Loading Gemma3-1B with optional LoRA weights\n",
    "2. **Math Reasoning**: Solving arithmetic and word problems\n",
    "3. **Logic Problems**: Deductive reasoning and puzzles\n",
    "4. **Science Explanations**: Physics, biology, and natural phenomena\n",
    "5. **Domain Applications**: Medical, legal, coding, and finance assistants\n",
    "6. **Creative Reasoning**: Hypothetical scenarios and system design\n",
    "7. **Generation Strategies**: Comparing different temperature/sampling settings\n",
    "8. **GRPO Training**: Understanding the training methodology\n",
    "9. **Evaluation**: Measuring accuracy on sample problems\n",
    "\n",
    "### Next Steps:\n",
    "- Fine-tune with your own dataset using Tunix on TPU/Colab\n",
    "- Export model for mobile deployment (Cactus format)\n",
    "- Extend to domain-specific applications\n",
    "\n",
    "### Resources:\n",
    "- [Tunix Documentation](https://tunix.readthedocs.io)\n",
    "- [Gemma Model Card](https://ai.google.dev/gemma)\n",
    "- [GRPO Paper](https://arxiv.org/abs/2402.03300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
