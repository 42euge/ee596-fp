{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v42.0.0.3 Gemma3-1B GRPO Training Notebook\n",
    "\n",
    "This notebook trains Gemma3-1B with GRPO (Group Relative Policy Optimization) for improved reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with TPU runtime (recommended) or GPU\n",
    "- HuggingFace account with Gemma access\n",
    "\n",
    "**Output:**\n",
    "- LoRA checkpoint files that can be downloaded and used locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec('tensorflow') is None:\n",
    "  print(\"Installing required packages...\")\n",
    "  %pip install -q dotenv\n",
    "  %pip install -q kagglehub\n",
    "  %pip install -q ipywidgets\n",
    "  %pip install -q tensorflow\n",
    "  %pip install -q tensorflow_datasets\n",
    "  %pip install -q tensorboardX\n",
    "  %pip install -q transformers\n",
    "  %pip install -q grain\n",
    "  %pip install -q git+https://github.com/jax-ml/jax\n",
    "  %pip install git+https://github.com/google/tunix\n",
    "  %pip install git+https://github.com/google/qwix\n",
    "  %pip uninstall -q flax -y\n",
    "  %pip install git+https://github.com/google/flax\n",
    "  %pip install -q huggingface_hub\n",
    "  %pip install -q datasets\n",
    "  %pip install -q 'numpy>2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add tunrex from github\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec('tunrex') is None:\n",
    "  %pip install git+https://github.com/42euge/TunRex.git@feature/models-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# =============================================================================\n",
    "# EDIT THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Training settings\n",
    "NUM_BATCHES = 500          # Number of training batches (500 = ~30 min on TPU)\n",
    "LEARNING_RATE = 3e-6       # Learning rate\n",
    "LORA_RANK = 64             # LoRA rank\n",
    "LORA_ALPHA = 64.0          # LoRA alpha\n",
    "\n",
    "# Dataset settings\n",
    "USE_OPENRUBRICS = True     # Use OpenRubrics dataset\n",
    "OPENRUBRICS_MAX = 2000     # Max examples from OpenRubrics\n",
    "\n",
    "# Checkpoint settings\n",
    "SAVE_TO_DRIVE = False       # Save checkpoints to Google Drive\n",
    "EXPERIMENT_NAME = \"gemma3_grpo_reasoning\"\n",
    "\n",
    "# =============================================================================\n",
    "# CREDENTIALS - Three options (in order of priority):\n",
    "#   1. Literal values below (uncomment and fill in)\n",
    "#   2. Google Colab secrets (set via key icon in sidebar)\n",
    "#   3. Kaggle secrets (when running on Kaggle)\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "# Option 1: Literal values (uncomment and fill in your credentials)\n",
    "os.environ['WANDB_API_KEY'] = 'REDACTED_WANDB_KEY'\n",
    "os.environ['KAGGLE_USERNAME'] = 'eugenio0'\n",
    "os.environ['KAGGLE_KEY'] = 'REDACTED_KAGGLE_KEY'\n",
    "\n",
    "# Option 2 & 3: Try secrets providers if env vars not already set\n",
    "if not os.environ.get('KAGGLE_USERNAME'):\n",
    "    # Try Google Colab secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "        os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
    "        os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
    "        print(\"Using Google Colab secrets\")\n",
    "    except (ImportError, ModuleNotFoundError):\n",
    "        # Fall back to Kaggle secrets\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            secrets = UserSecretsClient()\n",
    "            os.environ['WANDB_API_KEY'] = secrets.get_secret('WANDB_API_KEY')\n",
    "            os.environ['KAGGLE_USERNAME'] = secrets.get_secret('KAGGLE_USERNAME')\n",
    "            os.environ['KAGGLE_KEY'] = secrets.get_secret('KAGGLE_KEY')\n",
    "            print(\"Using Kaggle secrets\")\n",
    "        except (ImportError, ModuleNotFoundError):\n",
    "            print(\"WARNING: No credentials found. Either:\")\n",
    "            print(\"  1. Uncomment and fill in literal values above\")\n",
    "            print(\"  2. Set Colab secrets (key icon in sidebar)\")\n",
    "            print(\"  3. Set Kaggle secrets\")\n",
    "else:\n",
    "    print(\"Using literal credentials from environment\")\n",
    "\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  Batches: {NUM_BATCHES}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Kaggle user: {os.environ.get('KAGGLE_USERNAME', 'not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended)\n",
    "if SAVE_TO_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = f\"/content/drive/MyDrive/{EXPERIMENT_NAME}/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt configuration\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think carefully and show your detailed reasoning step-by-step. Place your reasoning between {REASONING_START} and {REASONING_END}. After completing your reasoning, provide the final answer between {SOLUTION_START} and {SOLUTION_END}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "def format_prompt(question, rubric=None):\n",
    "    rubric_block = f\"\\nRubric:\\n{rubric}\\n\\n\" if rubric else \"\"\n",
    "    return TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=f\"{rubric_block}{question}\",\n",
    "    )\n",
    "\n",
    "print(\"Prompt template configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using TunRex\n",
    "from tunrex import load_openrubrics\n",
    "\n",
    "if USE_OPENRUBRICS:\n",
    "    train_data = load_openrubrics(max_examples=OPENRUBRICS_MAX)\n",
    "else:\n",
    "    train_data = []  # Add GSM8K loading if needed\n",
    "\n",
    "print(f\"\\nTotal training examples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grain dataset\n",
    "def create_dataset(data):\n",
    "    return (\n",
    "        grain.MapDataset.source(data)\n",
    "        .shuffle(seed=42)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"prompts\": format_prompt(x[\"question\"], x.get(\"rubric\")),\n",
    "                \"question\": x[\"question\"],\n",
    "                \"rubric\": x.get(\"rubric\", \"\"),\n",
    "                \"reference_response\": x.get(\"reference_response\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(train_data) * 0.9)\n",
    "train_split = train_data[:split_idx]\n",
    "test_split = train_data[split_idx:]\n",
    "\n",
    "train_dataset = create_dataset(train_split)\n",
    "test_dataset = create_dataset(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to add a previous checkpoint loading mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Gemma checkpoint using TunRex\n",
    "from tunrex import prepare_gemma_checkpoint\n",
    "\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/intermediate_ckpt\"\n",
    "ckpt_path, MODEL_CP_PATH, tokenizer = prepare_gemma_checkpoint(\n",
    "    ckpt_dir=INTERMEDIATE_CKPT_DIR,\n",
    ")\n",
    "print(\"Base model checkpoint prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference model using TunRex\n",
    "from tunrex import get_gemma_ref_model\n",
    "\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "    ckpt_path=ckpt_path,\n",
    "    model_checkpoint_path=MODEL_CP_PATH,\n",
    ")\n",
    "print(\"Reference model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA model using TunRex\n",
    "from tunrex import get_lora_model\n",
    "\n",
    "lora_policy = get_lora_model(\n",
    "    base_model=ref_model,\n",
    "    mesh=mesh,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    ")\n",
    "print(f\"LoRA model created with rank={LORA_RANK}, alpha={LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import difflib\n",
    "from collections import Counter\n",
    "\n",
    "# Format matching regex\n",
    "match_format = re.compile(\n",
    "    rf\"{REASONING_START}.*?{REASONING_END}.*?{SOLUTION_START}.*?{SOLUTION_END}\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward for proper format usage.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if match_format.search(completion):\n",
    "            scores.append(2.0)\n",
    "        elif REASONING_START in completion or SOLUTION_START in completion:\n",
    "            scores.append(0.5)\n",
    "        else:\n",
    "            scores.append(-1.0)\n",
    "    return scores\n",
    "\n",
    "def rubric_overlap_score(response, rubric_text):\n",
    "    \"\"\"Calculate rubric overlap with TF-IDF weighting.\"\"\"\n",
    "    def tokenize(text):\n",
    "        text = text.lower()\n",
    "        for ch in string.punctuation:\n",
    "            text = text.replace(ch, \" \")\n",
    "        return [t for t in text.split() if len(t) > 2]\n",
    "    \n",
    "    rubric_tokens = tokenize(rubric_text)\n",
    "    response_tokens = set(tokenize(response))\n",
    "    \n",
    "    if not rubric_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    token_counts = Counter(rubric_tokens)\n",
    "    weighted_matches = sum(\n",
    "        1.0 / token_counts[t] for t in response_tokens if t in token_counts\n",
    "    )\n",
    "    max_score = sum(1.0 / c for c in token_counts.values())\n",
    "    \n",
    "    coverage = weighted_matches / max_score if max_score > 0 else 0.0\n",
    "    return coverage * 10.0\n",
    "\n",
    "def rar_reward(prompts, completions, rubric=None, reference_response=None, **kwargs):\n",
    "    \"\"\"Rubric-as-Reward scoring.\"\"\"\n",
    "    rubrics = rubric or [\"\"] * len(completions)\n",
    "    references = reference_response or [\"\"] * len(completions)\n",
    "    \n",
    "    rewards = []\n",
    "    for response, rub, ref in zip(completions, rubrics, references):\n",
    "        # Rubric overlap (0-10)\n",
    "        r_score = rubric_overlap_score(response, rub) if rub else 0.0\n",
    "        \n",
    "        # Reference similarity (0-5)\n",
    "        f_score = difflib.SequenceMatcher(None, ref, response).ratio() * 5.0 if ref else 0.0\n",
    "        \n",
    "        rewards.append(r_score + f_score)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler for generation\n",
    "sampler = sampler_lib.Sampler(\n",
    "    model=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    "    mesh=mesh,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "MAX_STEPS = int(NUM_BATCHES * 0.94)  # With train fraction\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "\n",
    "# Optimizer with warmup + cosine decay\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=0.9,\n",
    "    b2=0.99,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(max_norm=0.1),\n",
    "    optimizer,\n",
    ")\n",
    "\n",
    "print(f\"Max steps: {MAX_STEPS}, Warmup: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=2,\n",
    "    num_iterations=1,\n",
    "    beta=0.08,\n",
    "    epsilon=0.2,\n",
    ")\n",
    "\n",
    "# Cluster configuration\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    max_prompt_length=256,\n",
    "    total_generation_steps=512,\n",
    ")\n",
    "\n",
    "# Data iterator config\n",
    "data_iter_config = base_rollout.DataIteratorConfig(\n",
    "    batch_size=2,\n",
    "    num_batches=NUM_BATCHES,\n",
    ")\n",
    "\n",
    "print(\"GRPO config created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    config=cluster_config,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    mesh=mesh,\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "# Checkpoint options\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=100,\n",
    "    max_to_keep=3,\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/grpo\",\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "print(\"RL cluster created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPO trainer\n",
    "reward_fns = [match_format_reward, rar_reward]\n",
    "\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=reward_fns,\n",
    "    algo_config=grpo_config,\n",
    "    optimizer=optimizer,\n",
    "    ckpt_dir=CHECKPOINT_DIR,\n",
    "    ckpt_options=checkpointing_options,\n",
    "    metrics_logger_options=metrics_logging_options,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer created.\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterator\n",
    "train_iter = train_dataset.batch(data_iter_config.batch_size)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting GRPO Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batches: {NUM_BATCHES}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "grpo_trainer.train(\n",
    "    policy=lora_policy,\n",
    "    data_iterator=train_iter,\n",
    "    data_iterator_config=data_iter_config,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Checkpoint for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import glob\n",
    "\n",
    "ckpt_dirs = sorted(glob.glob(f\"{CHECKPOINT_DIR}/actor/*/\"))\n",
    "if ckpt_dirs:\n",
    "    latest_ckpt = ckpt_dirs[-1]\n",
    "    print(f\"Latest checkpoint: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace format for local use\n",
    "# This creates adapter files compatible with PEFT\n",
    "\n",
    "EXPORT_DIR = f\"{CHECKPOINT_DIR}/hf_lora\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Save LoRA state\n",
    "lora_state = nnx.state(lora_policy)\n",
    "\n",
    "# Filter to only LoRA parameters\n",
    "lora_params = {}\n",
    "def extract_lora(path, value):\n",
    "    path_str = \".\".join(str(p) for p in path)\n",
    "    if \"lora\" in path_str.lower():\n",
    "        lora_params[path_str] = value\n",
    "\n",
    "jax.tree_util.tree_map_with_path(extract_lora, lora_state)\n",
    "\n",
    "print(f\"Found {len(lora_params)} LoRA parameters\")\n",
    "print(f\"\\nCheckpoint saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\nTo use locally:\")\n",
    "print(f\"1. Download the checkpoint folder from Google Drive\")\n",
    "print(f\"2. Place in your local checkpoints/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "if SAVE_TO_DRIVE:\n",
    "    !cd {CHECKPOINT_DIR} && zip -r checkpoint_export.zip actor/\n",
    "    print(f\"\\nZipped checkpoint: {CHECKPOINT_DIR}/checkpoint_export.zip\")\n",
    "    print(\"Download this file from Google Drive and extract to checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_question = \"A store sells apples for $2 each. If I buy 5 apples, how much do I spend?\"\n",
    "test_prompt = format_prompt(test_question)\n",
    "\n",
    "print(\"Testing trained model...\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print()\n",
    "\n",
    "response = sampler(\n",
    "    [test_prompt],\n",
    "    total_generation_steps=256,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")[0]\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your checkpoints are saved. To use them locally:\n",
    "\n",
    "1. Download `checkpoint_export.zip` from Google Drive\n",
    "2. Extract to your local `checkpoints/` folder\n",
    "3. Run: `python demo/demo.py --checkpoint ./checkpoints/actor/<step>/model_params`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "kernelspec": {
   "display_name": "tunix-hackathon-google (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
