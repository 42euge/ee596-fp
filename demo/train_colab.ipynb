{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3-1B GRPO Training Notebook\n",
    "\n",
    "This notebook trains Gemma3-1B with GRPO (Group Relative Policy Optimization) for improved reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with TPU runtime (recommended) or GPU\n",
    "- HuggingFace account with Gemma access\n",
    "\n",
    "**Output:**\n",
    "- LoRA checkpoint files that can be downloaded and used locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jax 0.8.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.8.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-metrax 0.2.4 requires numpy>=2.1.3, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 2.30.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting git+https://github.com/google/tunix.git\n",
      "  Cloning https://github.com/google/tunix.git to /tmp/pip-req-build-om1dl6e6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google/tunix.git /tmp/pip-req-build-om1dl6e6\n",
      "  Resolved https://github.com/google/tunix.git to commit 1ccb62c14bdf93b652e5d836461b17aa35322a09\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (4.0.0)\n",
      "Requirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (2025.3.0)\n",
      "Requirement already satisfied: google-metrax>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.2.4)\n",
      "Requirement already satisfied: grain in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.2.15)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.36.0)\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.3.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (3.1.6)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.3.13)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.63.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (2.3.0)\n",
      "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (2.10)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (1.2.1)\n",
      "Requirement already satisfied: qwix in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.1.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.2.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (1.14.0)\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (4.9.9)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (4.67.1)\n",
      "Requirement already satisfied: transformers<=4.57.1 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (4.57.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.6) (0.1.9)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (1.26.4)\n",
      "Requirement already satisfied: jax>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (0.8.1)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (1.1.2)\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (0.2.6)\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (0.11.30)\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (0.1.79)\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (14.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (4.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (6.0.3)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.6) (0.1.10)\n",
      "Requirement already satisfied: clu>=0.0.12 in /usr/local/lib/python3.12/dist-packages (from google-metrax>=0.2.3->google-tunix==0.1.6) (0.0.12)\n",
      "Collecting numpy>=1.23.2 (from flax>=0.11.1->google-tunix==0.1.6)\n",
      "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: tensorboardx>=2.6.4 in /usr/local/lib/python3.12/dist-packages (from google-metrax>=0.2.3->google-tunix==0.1.6) (2.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.1->google-tunix==0.1.6) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->google-tunix==0.1.6) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.6) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.6) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.6) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.6) (1.4.0)\n",
      "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.6) (0.8.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.6) (3.1.2)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.6) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.6) (5.29.5)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping->google-tunix==0.1.6) (0.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->google-tunix==0.1.6) (3.0.3)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->google-tunix==0.1.6) (0.46.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->google-tunix==0.1.6) (4.9.3)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from qwix->google-tunix==0.1.6) (0.8.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->google-tunix==0.1.6) (1.3.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (0.1.9)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (4.2.2)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (2.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (5.9.5)\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (1.17.2)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (3.2.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.6) (2.0.1)\n",
      "Requirement already satisfied: ml-collections in /usr/local/lib/python3.12/dist-packages (from clu>=0.0.12->google-metrax>=0.2.3->google-tunix==0.1.6) (1.1.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.6) (0.8.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.6) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.6) (3.23.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (3.13.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax>=0.11.1->google-tunix==0.1.6) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax>=0.11.1->google-tunix==0.1.6) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax>=0.11.1->google-tunix==0.1.6) (1.16.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<=4.57.1->google-tunix==0.1.6) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<=4.57.1->google-tunix==0.1.6) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<=4.57.1->google-tunix==0.1.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<=4.57.1->google-tunix==0.1.6) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.6) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.6) (2.19.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow_datasets->google-tunix==0.1.6) (25.4.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.11.1->google-tunix==0.1.6) (0.1.90)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.6) (1.6.0)\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.6) (24.1.0)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.6) (4.14.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.6) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.6) (2025.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from promise->tensorflow_datasets->google-tunix==0.1.6) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.6) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.6) (1.72.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->google-tunix==0.1.6) (1.22.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.6) (75.2.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.6) (0.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.6) (0.1.2)\n",
      "Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
      "bigframes 2.30.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.3.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ce07700d43214b1c99671898533feea1",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tunrex 0.1.0\n",
      "Uninstalling tunrex-0.1.0:\n",
      "  Successfully uninstalled tunrex-0.1.0\n",
      "Collecting git+https://github.com/42euge/TunRex.git@feature/models-api\n",
      "  Cloning https://github.com/42euge/TunRex.git (to revision feature/models-api) to /tmp/pip-req-build-3zylil84\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/42euge/TunRex.git /tmp/pip-req-build-3zylil84\n",
      "  Running command git checkout -b feature/models-api --track origin/feature/models-api\n",
      "  Switched to a new branch 'feature/models-api'\n",
      "  Branch 'feature/models-api' set up to track remote branch 'feature/models-api' from 'origin'.\n",
      "  Resolved https://github.com/42euge/TunRex.git to commit 325dfe9f0852d7ce4e40de9b07c9a20002930135\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (8.3.1)\n",
      "Requirement already satisfied: textual>=0.89.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (6.10.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=14.0.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (18.1.0)\n",
      "Requirement already satisfied: jsonlines>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: grain>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (0.2.15)\n",
      "Requirement already satisfied: datasets>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: kagglehub>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from tunrex==0.1.0) (0.3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (2.3.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.0->tunrex==0.1.0) (6.0.3)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from grain>=0.2.0->tunrex==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from grain>=0.2.0->tunrex==0.1.0) (0.8.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from grain>=0.2.0->tunrex==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from grain>=0.2.0->tunrex==0.1.0) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/dist-packages (from grain>=0.2.0->tunrex==0.1.0) (5.29.5)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=4.0.0->tunrex==0.1.0) (25.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->tunrex==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->tunrex==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->tunrex==0.1.0) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py[linkify]>=2.1.0->textual>=0.89.0->tunrex==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.12/dist-packages (from textual>=0.89.0->tunrex==0.1.0) (0.5.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from textual>=0.89.0->tunrex==0.1.0) (4.5.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.19.2 in /usr/local/lib/python3.12/dist-packages (from textual>=0.89.0->tunrex==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: rich>=14.2.0 in /usr/local/lib/python3.12/dist-packages (from textual>=0.89.0->tunrex==0.1.0) (14.2.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from textual>=0.89.0->tunrex==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.14.0->tunrex==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.1.0->markdown-it-py[linkify]>=2.1.0->textual>=0.89.0->tunrex==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py[linkify]>=2.1.0->textual>=0.89.0->tunrex==0.1.0) (2.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->tunrex==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.0->tunrex==0.1.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.0->tunrex==0.1.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.0->tunrex==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.0->tunrex==0.1.0) (2025.11.12)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->grain>=0.2.0->tunrex==0.1.0) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->grain>=0.2.0->tunrex==0.1.0) (3.23.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.0->tunrex==0.1.0) (1.22.0)\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.12/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.1.0->textual>=0.89.0->tunrex==0.1.0) (1.0.3)\n",
      "Building wheels for collected packages: tunrex\n",
      "  Building wheel for tunrex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tunrex: filename=tunrex-0.1.0-py3-none-any.whl size=36404 sha256=ed8deda0e2ad485df2c2fd24e35819f742a47815c756bfaa1c1655e39dd96fd6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4zr23qi/wheels/3c/d9/34/2ffdc80f195a4c95feeecf9fb075bd426a4541c0aa5c68d3ab\n",
      "Successfully built tunrex\n",
      "Installing collected packages: tunrex\n",
      "Successfully installed tunrex-0.1.0\n",
      "Collecting flax==0.12.0\n",
      "  Using cached flax-0.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (2.3.5)\n",
      "Requirement already satisfied: jax>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.8.1)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (1.1.2)\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.2.6)\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.11.30)\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.1.79)\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (14.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (4.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (6.0.3)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.1.10)\n",
      "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.8.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (1.16.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.12.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.12.0) (2.19.2)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->flax==0.12.0) (1.4.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax==0.12.0) (0.1.90)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.13.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.6.0)\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (24.1.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (5.29.5)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (4.14.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (3.20.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (5.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax==0.12.0) (75.2.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax==0.12.0) (0.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.12.0) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (2025.3.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (3.23.0)\n",
      "Using cached flax-0.12.0-py3-none-any.whl (466 kB)\n",
      "Installing collected packages: flax\n",
      "Successfully installed flax-0.12.0\n",
      "\n",
      "============================================================\n",
      "Installation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q kagglehub\n",
    "!pip install -q datasets\n",
    "!pip install -q wandb\n",
    "!pip install -q \"numpy<2.0\"\n",
    "!pip install git+https://github.com/google/tunix.git\n",
    "# Force fresh install of TunRex (pip caches aggressively)\n",
    "!pip uninstall -y tunrex 2>/dev/null || true\n",
    "!pip install --no-cache-dir git+https://github.com/42euge/TunRex.git@feature/models-api\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "!pip install -q 'transformers<=4.57.1'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Installation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using literal credentials from environment\n",
      "\n",
      "Training config:\n",
      "  Batches: 500\n",
      "  Learning rate: 3e-06\n",
      "  LoRA rank: 64\n",
      "  Kaggle user: eugenio0\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# =============================================================================\n",
    "# EDIT THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Training settings\n",
    "NUM_BATCHES = 500          # Number of training batches (500 = ~30 min on TPU)\n",
    "LEARNING_RATE = 3e-6       # Learning rate\n",
    "LORA_RANK = 64             # LoRA rank\n",
    "LORA_ALPHA = 64.0          # LoRA alpha\n",
    "\n",
    "# Dataset settings\n",
    "USE_OPENRUBRICS = True     # Use OpenRubrics dataset\n",
    "OPENRUBRICS_MAX = 2000     # Max examples from OpenRubrics\n",
    "\n",
    "# Checkpoint settings\n",
    "SAVE_TO_DRIVE = False       # Save checkpoints to Google Drive\n",
    "EXPERIMENT_NAME = \"gemma3_grpo_reasoning\"\n",
    "\n",
    "# =============================================================================\n",
    "# CREDENTIALS - Three options (in order of priority):\n",
    "#   1. Literal values below (uncomment and fill in)\n",
    "#   2. Google Colab secrets (set via key icon in sidebar)\n",
    "#   3. Kaggle secrets (when running on Kaggle)\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "# Option 1: Literal values (uncomment and fill in your credentials)\n",
    "os.environ['WANDB_API_KEY'] = '92c370d749b4a72da2eb10cb156cf0aa4eef05ef'\n",
    "os.environ['KAGGLE_USERNAME'] = 'eugenio0'\n",
    "os.environ['KAGGLE_KEY'] = 'KGAT_db78f48386586bd20c8694d71b859355'\n",
    "\n",
    "# Option 2 & 3: Try secrets providers if env vars not already set\n",
    "if not os.environ.get('KAGGLE_USERNAME'):\n",
    "    # Try Google Colab secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "        os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
    "        os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
    "        print(\"Using Google Colab secrets\")\n",
    "    except (ImportError, ModuleNotFoundError):\n",
    "        # Fall back to Kaggle secrets\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            secrets = UserSecretsClient()\n",
    "            os.environ['WANDB_API_KEY'] = secrets.get_secret('WANDB_API_KEY')\n",
    "            os.environ['KAGGLE_USERNAME'] = secrets.get_secret('KAGGLE_USERNAME')\n",
    "            os.environ['KAGGLE_KEY'] = secrets.get_secret('KAGGLE_KEY')\n",
    "            print(\"Using Kaggle secrets\")\n",
    "        except (ImportError, ModuleNotFoundError):\n",
    "            print(\"WARNING: No credentials found. Either:\")\n",
    "            print(\"  1. Uncomment and fill in literal values above\")\n",
    "            print(\"  2. Set Colab secrets (key icon in sidebar)\")\n",
    "            print(\"  3. Set Kaggle secrets\")\n",
    "else:\n",
    "    print(\"Using literal credentials from environment\")\n",
    "\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  Batches: {NUM_BATCHES}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Kaggle user: {os.environ.get('KAGGLE_USERNAME', 'not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended)\n",
    "if SAVE_TO_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = f\"/content/drive/MyDrive/{EXPERIMENT_NAME}/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt configuration\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think carefully and show your detailed reasoning step-by-step. Place your reasoning between {REASONING_START} and {REASONING_END}. After completing your reasoning, provide the final answer between {SOLUTION_START} and {SOLUTION_END}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "def format_prompt(question, rubric=None):\n",
    "    rubric_block = f\"\\nRubric:\\n{rubric}\\n\\n\" if rubric else \"\"\n",
    "    return TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=f\"{rubric_block}{question}\",\n",
    "    )\n",
    "\n",
    "print(\"Prompt template configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 examples from OpenRubrics (train)\n",
      "\n",
      "Total training examples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load dataset using TunRex\n",
    "from tunrex import load_openrubrics\n",
    "\n",
    "if USE_OPENRUBRICS:\n",
    "    train_data = load_openrubrics(max_examples=OPENRUBRICS_MAX)\n",
    "else:\n",
    "    train_data = []  # Add GSM8K loading if needed\n",
    "\n",
    "print(f\"\\nTotal training examples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1800, Test: 200\n"
     ]
    }
   ],
   "source": [
    "# Create grain dataset\n",
    "def create_dataset(data):\n",
    "    return (\n",
    "        grain.MapDataset.source(data)\n",
    "        .shuffle(seed=42)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"prompts\": format_prompt(x[\"question\"], x.get(\"rubric\")),\n",
    "                \"question\": x[\"question\"],\n",
    "                \"rubric\": x.get(\"rubric\", \"\"),\n",
    "                \"reference_response\": x.get(\"reference_response\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(train_data) * 0.9)\n",
    "train_split = train_data[:split_idx]\n",
    "test_split = train_data[split_idx:]\n",
    "\n",
    "train_dataset = create_dataset(train_split)\n",
    "test_dataset = create_dataset(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: eugenio0\n"
     ]
    },
    {
     "ename": "ColabHTTPError",
     "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-3/flax/gemma3-1b-it\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mcolab_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: http://172.28.0.1:8011/kagglehub/models/mount",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mColabHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2500110642.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mMODEL_CP_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/gemma-3/flax/gemma3-1b-it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model path: {MODEL_CP_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/models.py\u001b[0m in \u001b[0;36mmodel_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_model_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Model: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/colab_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColabClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_MOUNT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, data, handle_path, resource_handle)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mHTTP_STATUS_404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mcolab_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mcolab_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Default handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mColabHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mColabHTTPError\u001b[0m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma-3/flax/gemma3-1b-it\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
     ]
    }
   ],
   "source": [
    "# TODO: change to add a previous checkpoint loading mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Gemma checkpoint using TunRex\n",
    "from tunrex import prepare_gemma_checkpoint\n",
    "\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/intermediate_ckpt\"\n",
    "ckpt_path, MODEL_CP_PATH, tokenizer = prepare_gemma_checkpoint(\n",
    "    ckpt_dir=INTERMEDIATE_CKPT_DIR,\n",
    ")\n",
    "print(\"Base model checkpoint prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference model using TunRex\n",
    "from tunrex import get_gemma_ref_model\n",
    "\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "    ckpt_path=ckpt_path,\n",
    "    model_checkpoint_path=MODEL_CP_PATH,\n",
    ")\n",
    "print(\"Reference model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA model using TunRex\n",
    "from tunrex import get_lora_model\n",
    "\n",
    "lora_policy = get_lora_model(\n",
    "    base_model=ref_model,\n",
    "    mesh=mesh,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    ")\n",
    "print(f\"LoRA model created with rank={LORA_RANK}, alpha={LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import difflib\n",
    "from collections import Counter\n",
    "\n",
    "# Format matching regex\n",
    "match_format = re.compile(\n",
    "    rf\"{REASONING_START}.*?{REASONING_END}.*?{SOLUTION_START}.*?{SOLUTION_END}\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward for proper format usage.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if match_format.search(completion):\n",
    "            scores.append(2.0)\n",
    "        elif REASONING_START in completion or SOLUTION_START in completion:\n",
    "            scores.append(0.5)\n",
    "        else:\n",
    "            scores.append(-1.0)\n",
    "    return scores\n",
    "\n",
    "def rubric_overlap_score(response, rubric_text):\n",
    "    \"\"\"Calculate rubric overlap with TF-IDF weighting.\"\"\"\n",
    "    def tokenize(text):\n",
    "        text = text.lower()\n",
    "        for ch in string.punctuation:\n",
    "            text = text.replace(ch, \" \")\n",
    "        return [t for t in text.split() if len(t) > 2]\n",
    "    \n",
    "    rubric_tokens = tokenize(rubric_text)\n",
    "    response_tokens = set(tokenize(response))\n",
    "    \n",
    "    if not rubric_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    token_counts = Counter(rubric_tokens)\n",
    "    weighted_matches = sum(\n",
    "        1.0 / token_counts[t] for t in response_tokens if t in token_counts\n",
    "    )\n",
    "    max_score = sum(1.0 / c for c in token_counts.values())\n",
    "    \n",
    "    coverage = weighted_matches / max_score if max_score > 0 else 0.0\n",
    "    return coverage * 10.0\n",
    "\n",
    "def rar_reward(prompts, completions, rubric=None, reference_response=None, **kwargs):\n",
    "    \"\"\"Rubric-as-Reward scoring.\"\"\"\n",
    "    rubrics = rubric or [\"\"] * len(completions)\n",
    "    references = reference_response or [\"\"] * len(completions)\n",
    "    \n",
    "    rewards = []\n",
    "    for response, rub, ref in zip(completions, rubrics, references):\n",
    "        # Rubric overlap (0-10)\n",
    "        r_score = rubric_overlap_score(response, rub) if rub else 0.0\n",
    "        \n",
    "        # Reference similarity (0-5)\n",
    "        f_score = difflib.SequenceMatcher(None, ref, response).ratio() * 5.0 if ref else 0.0\n",
    "        \n",
    "        rewards.append(r_score + f_score)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler for generation\n",
    "sampler = sampler_lib.Sampler(\n",
    "    model=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    "    mesh=mesh,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "MAX_STEPS = int(NUM_BATCHES * 0.94)  # With train fraction\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "\n",
    "# Optimizer with warmup + cosine decay\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=0.9,\n",
    "    b2=0.99,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(max_norm=0.1),\n",
    "    optimizer,\n",
    ")\n",
    "\n",
    "print(f\"Max steps: {MAX_STEPS}, Warmup: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=2,\n",
    "    num_iterations=1,\n",
    "    beta=0.08,\n",
    "    epsilon=0.2,\n",
    ")\n",
    "\n",
    "# Cluster configuration\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    max_prompt_length=256,\n",
    "    total_generation_steps=512,\n",
    ")\n",
    "\n",
    "# Data iterator config\n",
    "data_iter_config = base_rollout.DataIteratorConfig(\n",
    "    batch_size=2,\n",
    "    num_batches=NUM_BATCHES,\n",
    ")\n",
    "\n",
    "print(\"GRPO config created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    config=cluster_config,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    mesh=mesh,\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "# Checkpoint options\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=100,\n",
    "    max_to_keep=3,\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/grpo\",\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "print(\"RL cluster created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPO trainer\n",
    "reward_fns = [match_format_reward, rar_reward]\n",
    "\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=reward_fns,\n",
    "    algo_config=grpo_config,\n",
    "    optimizer=optimizer,\n",
    "    ckpt_dir=CHECKPOINT_DIR,\n",
    "    ckpt_options=checkpointing_options,\n",
    "    metrics_logger_options=metrics_logging_options,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer created.\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterator\n",
    "train_iter = train_dataset.batch(data_iter_config.batch_size)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting GRPO Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batches: {NUM_BATCHES}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "grpo_trainer.train(\n",
    "    policy=lora_policy,\n",
    "    data_iterator=train_iter,\n",
    "    data_iterator_config=data_iter_config,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Checkpoint for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import glob\n",
    "\n",
    "ckpt_dirs = sorted(glob.glob(f\"{CHECKPOINT_DIR}/actor/*/\"))\n",
    "if ckpt_dirs:\n",
    "    latest_ckpt = ckpt_dirs[-1]\n",
    "    print(f\"Latest checkpoint: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace format for local use\n",
    "# This creates adapter files compatible with PEFT\n",
    "\n",
    "EXPORT_DIR = f\"{CHECKPOINT_DIR}/hf_lora\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Save LoRA state\n",
    "lora_state = nnx.state(lora_policy)\n",
    "\n",
    "# Filter to only LoRA parameters\n",
    "lora_params = {}\n",
    "def extract_lora(path, value):\n",
    "    path_str = \".\".join(str(p) for p in path)\n",
    "    if \"lora\" in path_str.lower():\n",
    "        lora_params[path_str] = value\n",
    "\n",
    "jax.tree_util.tree_map_with_path(extract_lora, lora_state)\n",
    "\n",
    "print(f\"Found {len(lora_params)} LoRA parameters\")\n",
    "print(f\"\\nCheckpoint saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\nTo use locally:\")\n",
    "print(f\"1. Download the checkpoint folder from Google Drive\")\n",
    "print(f\"2. Place in your local checkpoints/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "if SAVE_TO_DRIVE:\n",
    "    !cd {CHECKPOINT_DIR} && zip -r checkpoint_export.zip actor/\n",
    "    print(f\"\\nZipped checkpoint: {CHECKPOINT_DIR}/checkpoint_export.zip\")\n",
    "    print(\"Download this file from Google Drive and extract to checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_question = \"A store sells apples for $2 each. If I buy 5 apples, how much do I spend?\"\n",
    "test_prompt = format_prompt(test_question)\n",
    "\n",
    "print(\"Testing trained model...\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print()\n",
    "\n",
    "response = sampler(\n",
    "    [test_prompt],\n",
    "    total_generation_steps=256,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")[0]\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your checkpoints are saved. To use them locally:\n",
    "\n",
    "1. Download `checkpoint_export.zip` from Google Drive\n",
    "2. Extract to your local `checkpoints/` folder\n",
    "3. Run: `python demo/demo.py --checkpoint ./checkpoints/actor/<step>/model_params`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
