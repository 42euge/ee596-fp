{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3-1B GRPO Training Notebook\n",
    "\n",
    "This notebook trains Gemma3-1B with GRPO (Group Relative Policy Optimization) for improved reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with TPU runtime (recommended) or GPU\n",
    "- HuggingFace account with Gemma access\n",
    "\n",
    "**Output:**\n",
    "- LoRA checkpoint files that can be downloaded and used locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q kagglehub\n",
    "!pip install -q datasets\n",
    "!pip install -q wandb\n",
    "!pip install -q \"numpy<2.0\"\n",
    "!pip install git+https://github.com/google/tunix.git\n",
    "!pip uninstall -q -y flax\n",
    "!pip install flax==0.12.0\n",
    "!pip install -q 'transformers<=4.57.1'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Installation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# =============================================================================\n",
    "# EDIT THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Training settings\n",
    "NUM_BATCHES = 500          # Number of training batches (500 = ~30 min on TPU)\n",
    "LEARNING_RATE = 3e-6       # Learning rate\n",
    "LORA_RANK = 64             # LoRA rank\n",
    "LORA_ALPHA = 64.0          # LoRA alpha\n",
    "\n",
    "# Dataset settings\n",
    "USE_OPENRUBRICS = True     # Use OpenRubrics dataset\n",
    "OPENRUBRICS_MAX = 2000     # Max examples from OpenRubrics\n",
    "\n",
    "# Checkpoint settings\n",
    "SAVE_TO_DRIVE = False       # Save checkpoints to Google Drive\n",
    "EXPERIMENT_NAME = \"gemma3_grpo_reasoning\"\n",
    "\n",
    "# =============================================================================\n",
    "# CREDENTIALS - Uses Colab Secrets or Kaggle Secrets\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "# Try Google Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
    "    os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
    "    print(\"Using Google Colab secrets\")\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    # Fall back to Kaggle secrets\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        os.environ['WANDB_API_KEY'] = secrets.get_secret('WANDB_API_KEY')\n",
    "        os.environ['KAGGLE_USERNAME'] = secrets.get_secret('KAGGLE_USERNAME')\n",
    "        os.environ['KAGGLE_KEY'] = secrets.get_secret('KAGGLE_KEY')\n",
    "        print(\"Using Kaggle secrets\")\n",
    "    except (ImportError, ModuleNotFoundError):\n",
    "        print(\"WARNING: No secrets provider found. Set environment variables manually:\")\n",
    "        print(\"  export WANDB_API_KEY=your_key\")\n",
    "        print(\"  export KAGGLE_USERNAME=your_username\")\n",
    "        print(\"  export KAGGLE_KEY=your_key\")\n",
    "\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  Batches: {NUM_BATCHES}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Kaggle user: {os.environ.get('KAGGLE_USERNAME', 'not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended)\n",
    "if SAVE_TO_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = f\"/content/drive/MyDrive/{EXPERIMENT_NAME}/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "    import os\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt configuration\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "SOLUTION_START = \"<answer>\"\n",
    "SOLUTION_END = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think carefully and show your detailed reasoning step-by-step. Place your reasoning between {REASONING_START} and {REASONING_END}. After completing your reasoning, provide the final answer between {SOLUTION_START} and {SOLUTION_END}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "def format_prompt(question, rubric=None):\n",
    "    rubric_block = f\"\\nRubric:\\n{rubric}\\n\\n\" if rubric else \"\"\n",
    "    return TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=f\"{rubric_block}{question}\",\n",
    "    )\n",
    "\n",
    "print(\"Prompt template configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openrubrics_dataset(max_examples=2000):\n",
    "    \"\"\"Load OpenRubrics dataset from HuggingFace.\"\"\"\n",
    "    print(\"Loading OpenRubrics dataset...\")\n",
    "    try:\n",
    "        raw_ds = load_dataset(\"OpenRubrics/OpenRubrics\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading OpenRubrics: {e}\")\n",
    "        return []\n",
    "    \n",
    "    columns = list(raw_ds.column_names)\n",
    "    \n",
    "    def pick_col(options):\n",
    "        for name in options:\n",
    "            if name in columns:\n",
    "                return name\n",
    "        return \"\"\n",
    "    \n",
    "    question_col = pick_col([\"instruction\", \"prompt\", \"question\", \"input\"])\n",
    "    rubric_col = pick_col([\"rubric\", \"scoring_rubric\", \"criteria\"])\n",
    "    response_col = pick_col([\"response\", \"model_answer\", \"answer\", \"output\"])\n",
    "    \n",
    "    data = []\n",
    "    for row in raw_ds:\n",
    "        question = row.get(question_col, \"\")\n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        rubric = row.get(rubric_col, \"\")\n",
    "        if isinstance(rubric, dict):\n",
    "            rubric = \"\\n\".join(f\"{k}: {v}\" for k, v in rubric.items() if v)\n",
    "        elif isinstance(rubric, list):\n",
    "            rubric = \"\\n\".join(str(r) for r in rubric if r)\n",
    "        \n",
    "        data.append({\n",
    "            \"question\": str(question),\n",
    "            \"rubric\": str(rubric) if rubric else \"\",\n",
    "            \"reference_response\": str(row.get(response_col, \"\")),\n",
    "        })\n",
    "        \n",
    "        if len(data) >= max_examples:\n",
    "            break\n",
    "    \n",
    "    print(f\"Loaded {len(data)} examples from OpenRubrics\")\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "if USE_OPENRUBRICS:\n",
    "    train_data = load_openrubrics_dataset(max_examples=OPENRUBRICS_MAX)\n",
    "else:\n",
    "    train_data = []  # Add GSM8K loading if needed\n",
    "\n",
    "print(f\"\\nTotal training examples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grain dataset\n",
    "def create_dataset(data):\n",
    "    return (\n",
    "        grain.MapDataset.source(data)\n",
    "        .shuffle(seed=42)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"prompts\": format_prompt(x[\"question\"], x.get(\"rubric\")),\n",
    "                \"question\": x[\"question\"],\n",
    "                \"rubric\": x.get(\"rubric\", \"\"),\n",
    "                \"reference_response\": x.get(\"reference_response\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(train_data) * 0.9)\n",
    "train_split = train_data[:split_idx]\n",
    "test_split = train_data[split_idx:]\n",
    "\n",
    "train_dataset = create_dataset(train_split)\n",
    "test_dataset = create_dataset(test_split)\n",
    "\n",
    "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Gemma3-1B checkpoint\n",
    "# Use Kaggle API directly to avoid Colab's proxy issues\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# Write credentials to kaggle.json (from environment variables set by secrets)\n",
    "kaggle_creds = {\n",
    "    \"username\": os.environ.get('KAGGLE_USERNAME', ''),\n",
    "    \"key\": os.environ.get('KAGGLE_KEY', '')\n",
    "}\n",
    "\n",
    "if not kaggle_creds['username'] or not kaggle_creds['key']:\n",
    "    raise ValueError(\"KAGGLE_USERNAME and KAGGLE_KEY must be set. Check your Colab/Kaggle secrets.\")\n",
    "\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "with open(kaggle_json_path, \"w\") as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "print(f\"Authenticated as: {kaggle_creds['username']}\")\n",
    "\n",
    "# Force kagglehub to not use Colab backend by patching before download\n",
    "import kagglehub.colab_cache_resolver\n",
    "kagglehub.colab_cache_resolver._COLAB_BACKEND_AVAILABLE = False\n",
    "\n",
    "# Also patch the env check\n",
    "import kagglehub.env\n",
    "kagglehub.env.is_colab_env = lambda: False\n",
    "\n",
    "# Clear any cached resolvers\n",
    "import importlib\n",
    "import kagglehub.models\n",
    "importlib.reload(kagglehub.models)\n",
    "\n",
    "import kagglehub\n",
    "MODEL_CP_PATH = kagglehub.model_download(\"google/gemma-3/flax/gemma3-1b-it\")\n",
    "print(f\"Model path: {MODEL_CP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "config = model.ModelConfig.gemma3_1b()\n",
    "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
    "tokenizer = params.create_tokenizer()\n",
    "\n",
    "# Save initial state\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/intermediate_ckpt/\"\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(INTERMEDIATE_CKPT_DIR + \"state\", state)\n",
    "\n",
    "del gemma\n",
    "gc.collect()\n",
    "print(\"Base model state saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference model\n",
    "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
    "\n",
    "def get_gemma_ref_model(ckpt_path):\n",
    "    model_config = model.ModelConfig.gemma3_1b()\n",
    "    abs_gemma = nnx.eval_shape(\n",
    "        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, model_config)\n",
    "    )\n",
    "    \n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    mesh = qwix.create_mesh(MESH)\n",
    "    sharding_rules = gemma.default_sharding_config(model_config, mesh)\n",
    "    shardings = qwix.make_shardings(abs_state, sharding_rules)\n",
    "    \n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "    \n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma_model = nnx.merge(graph_def, restored_params)\n",
    "    return gemma_model, mesh, model_config\n",
    "\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
    ")\n",
    "print(\"Reference model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA model\n",
    "def get_lora_model(base_model, mesh):\n",
    "    lora_provider = qwix.LoRAProvider(\n",
    "        rank=LORA_RANK,\n",
    "        alpha=LORA_ALPHA,\n",
    "        include=[r\".*attn.*\"],\n",
    "        exclude=[r\".*embed.*\"],\n",
    "    )\n",
    "    \n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "    \n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        sharded_state = jax.tree.map(jax.lax.with_sharding_constraint, state, state)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "    \n",
    "    return lora_model\n",
    "\n",
    "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
    "print(f\"LoRA model created with rank={LORA_RANK}, alpha={LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import difflib\n",
    "from collections import Counter\n",
    "\n",
    "# Format matching regex\n",
    "match_format = re.compile(\n",
    "    rf\"{REASONING_START}.*?{REASONING_END}.*?{SOLUTION_START}.*?{SOLUTION_END}\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward for proper format usage.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if match_format.search(completion):\n",
    "            scores.append(2.0)\n",
    "        elif REASONING_START in completion or SOLUTION_START in completion:\n",
    "            scores.append(0.5)\n",
    "        else:\n",
    "            scores.append(-1.0)\n",
    "    return scores\n",
    "\n",
    "def rubric_overlap_score(response, rubric_text):\n",
    "    \"\"\"Calculate rubric overlap with TF-IDF weighting.\"\"\"\n",
    "    def tokenize(text):\n",
    "        text = text.lower()\n",
    "        for ch in string.punctuation:\n",
    "            text = text.replace(ch, \" \")\n",
    "        return [t for t in text.split() if len(t) > 2]\n",
    "    \n",
    "    rubric_tokens = tokenize(rubric_text)\n",
    "    response_tokens = set(tokenize(response))\n",
    "    \n",
    "    if not rubric_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    token_counts = Counter(rubric_tokens)\n",
    "    weighted_matches = sum(\n",
    "        1.0 / token_counts[t] for t in response_tokens if t in token_counts\n",
    "    )\n",
    "    max_score = sum(1.0 / c for c in token_counts.values())\n",
    "    \n",
    "    coverage = weighted_matches / max_score if max_score > 0 else 0.0\n",
    "    return coverage * 10.0\n",
    "\n",
    "def rar_reward(prompts, completions, rubric=None, reference_response=None, **kwargs):\n",
    "    \"\"\"Rubric-as-Reward scoring.\"\"\"\n",
    "    rubrics = rubric or [\"\"] * len(completions)\n",
    "    references = reference_response or [\"\"] * len(completions)\n",
    "    \n",
    "    rewards = []\n",
    "    for response, rub, ref in zip(completions, rubrics, references):\n",
    "        # Rubric overlap (0-10)\n",
    "        r_score = rubric_overlap_score(response, rub) if rub else 0.0\n",
    "        \n",
    "        # Reference similarity (0-5)\n",
    "        f_score = difflib.SequenceMatcher(None, ref, response).ratio() * 5.0 if ref else 0.0\n",
    "        \n",
    "        rewards.append(r_score + f_score)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler for generation\n",
    "sampler = sampler_lib.Sampler(\n",
    "    model=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    "    mesh=mesh,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "MAX_STEPS = int(NUM_BATCHES * 0.94)  # With train fraction\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "\n",
    "# Optimizer with warmup + cosine decay\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=0.9,\n",
    "    b2=0.99,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(max_norm=0.1),\n",
    "    optimizer,\n",
    ")\n",
    "\n",
    "print(f\"Max steps: {MAX_STEPS}, Warmup: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=2,\n",
    "    num_iterations=1,\n",
    "    beta=0.08,\n",
    "    epsilon=0.2,\n",
    ")\n",
    "\n",
    "# Cluster configuration\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    max_prompt_length=256,\n",
    "    total_generation_steps=512,\n",
    ")\n",
    "\n",
    "# Data iterator config\n",
    "data_iter_config = base_rollout.DataIteratorConfig(\n",
    "    batch_size=2,\n",
    "    num_batches=NUM_BATCHES,\n",
    ")\n",
    "\n",
    "print(\"GRPO config created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    config=cluster_config,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    mesh=mesh,\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "# Checkpoint options\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=100,\n",
    "    max_to_keep=3,\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/grpo\",\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "print(\"RL cluster created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPO trainer\n",
    "reward_fns = [match_format_reward, rar_reward]\n",
    "\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=reward_fns,\n",
    "    algo_config=grpo_config,\n",
    "    optimizer=optimizer,\n",
    "    ckpt_dir=CHECKPOINT_DIR,\n",
    "    ckpt_options=checkpointing_options,\n",
    "    metrics_logger_options=metrics_logging_options,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer created.\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterator\n",
    "train_iter = train_dataset.batch(data_iter_config.batch_size)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting GRPO Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batches: {NUM_BATCHES}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "grpo_trainer.train(\n",
    "    policy=lora_policy,\n",
    "    data_iterator=train_iter,\n",
    "    data_iterator_config=data_iter_config,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Checkpoint for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import glob\n",
    "\n",
    "ckpt_dirs = sorted(glob.glob(f\"{CHECKPOINT_DIR}/actor/*/\"))\n",
    "if ckpt_dirs:\n",
    "    latest_ckpt = ckpt_dirs[-1]\n",
    "    print(f\"Latest checkpoint: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace format for local use\n",
    "# This creates adapter files compatible with PEFT\n",
    "\n",
    "EXPORT_DIR = f\"{CHECKPOINT_DIR}/hf_lora\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Save LoRA state\n",
    "lora_state = nnx.state(lora_policy)\n",
    "\n",
    "# Filter to only LoRA parameters\n",
    "lora_params = {}\n",
    "def extract_lora(path, value):\n",
    "    path_str = \".\".join(str(p) for p in path)\n",
    "    if \"lora\" in path_str.lower():\n",
    "        lora_params[path_str] = value\n",
    "\n",
    "jax.tree_util.tree_map_with_path(extract_lora, lora_state)\n",
    "\n",
    "print(f\"Found {len(lora_params)} LoRA parameters\")\n",
    "print(f\"\\nCheckpoint saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\nTo use locally:\")\n",
    "print(f\"1. Download the checkpoint folder from Google Drive\")\n",
    "print(f\"2. Place in your local checkpoints/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "if SAVE_TO_DRIVE:\n",
    "    !cd {CHECKPOINT_DIR} && zip -r checkpoint_export.zip actor/\n",
    "    print(f\"\\nZipped checkpoint: {CHECKPOINT_DIR}/checkpoint_export.zip\")\n",
    "    print(\"Download this file from Google Drive and extract to checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_question = \"A store sells apples for $2 each. If I buy 5 apples, how much do I spend?\"\n",
    "test_prompt = format_prompt(test_question)\n",
    "\n",
    "print(\"Testing trained model...\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print()\n",
    "\n",
    "response = sampler(\n",
    "    [test_prompt],\n",
    "    total_generation_steps=256,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")[0]\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your checkpoints are saved. To use them locally:\n",
    "\n",
    "1. Download `checkpoint_export.zip` from Google Drive\n",
    "2. Extract to your local `checkpoints/` folder\n",
    "3. Run: `python demo/demo.py --checkpoint ./checkpoints/actor/<step>/model_params`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
